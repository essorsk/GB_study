{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.metrics import f1_score\n",
    "import re\n",
    "import xgboost, textblob, string\n",
    "import emoji\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве заготовки для задания прогоним часть 2ого домашнего задания. Нам необходимо получить разреженные матрицы, используя CountVectorizer, TfidfVectorizer для 'tweet_stemmed' и 'tweet_lemmatized' столбцов (4 матрицы).\n",
    "\n",
    "#### Задание 1.\n",
    "Построим модель LogisticRegression, используя Bag-of-Words признаки для столбца combine_df['tweet_stemmed']. \n",
    "- Поделим Bag-of-Words признаки на train, test (train заканчивается на 31962 строке combine_df)\n",
    "- Ответами является столбец train_df['label']\n",
    "- Рассчитаем predict_proba, приведем prediction в в бинарный вид: если предсказание >= 0.3 то 1, иначе 0, тип заменим на int\n",
    "- Рассчитаем f1_score \n",
    "\n",
    "Повторим аналогично для столбца combine_df['tweet_lemmatized'].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>when father is dysfunctional and is so selfish...</td>\n",
       "      <td>[father, dysfunctional, selfish, drags, kids, ...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "      <td>[father, dysfunctional, selfish, drag, kid, dy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks for lyft credit cannot use cause they d...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...   \n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3    0.0                                bihday your majesty   \n",
       "\n",
       "                                         clean_tweet  \\\n",
       "0  when father is dysfunctional and is so selfish...   \n",
       "1  thanks for lyft credit cannot use cause they d...   \n",
       "2                                bihday your majesty   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [father, dysfunctional, selfish, drags, kids, ...   \n",
       "1  [thanks, lyft, credit, use, cause, offer, whee...   \n",
       "2                                  [bihday, majesty]   \n",
       "\n",
       "                                       tweet_stemmed  \\\n",
       "0  [father, dysfunct, selfish, drag, kid, dysfunc...   \n",
       "1  [thank, lyft, credit, use, caus, offer, wheelc...   \n",
       "2                                  [bihday, majesti]   \n",
       "\n",
       "                                    tweet_lemmatized  \n",
       "0  [father, dysfunctional, selfish, drag, kid, dy...  \n",
       "1  [thanks, lyft, credit, use, cause, offer, whee...  \n",
       "2                                  [bihday, majesty]  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Предобработанные данные взяты с прошлого ДЗ\n",
    "combine_df = pd.read_pickle('preproc.pkl')\n",
    "combine_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49159 entries, 0 to 49158\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   id                49159 non-null  int64  \n",
      " 1   label             31962 non-null  float64\n",
      " 2   tweet             49159 non-null  object \n",
      " 3   clean_tweet       49159 non-null  object \n",
      " 4   tweet_token       49159 non-null  object \n",
      " 5   tweet_stemmed     49159 non-null  object \n",
      " 6   tweet_lemmatized  49159 non-null  object \n",
      "dtypes: float64(1), int64(1), object(5)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "combine_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 31962 entries, 0 to 31961\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   id                31962 non-null  int64 \n",
      " 1   label             31962 non-null  int32 \n",
      " 2   tweet             31962 non-null  object\n",
      " 3   clean_tweet       31962 non-null  object\n",
      " 4   tweet_token       31962 non-null  object\n",
      " 5   tweet_stemmed     31962 non-null  object\n",
      " 6   tweet_lemmatized  31962 non-null  object\n",
      "dtypes: int32(1), int64(1), object(5)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "#Уберем пропуски, признаки преобразуем в целочисленные\n",
    "combine_df = combine_df.dropna()\n",
    "combine_df.label = combine_df.label.astype(int)\n",
    "combine_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23971"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Для разбивки используем 75Х25\n",
    "SSIZE = int(len(combine_df) * 0.75)\n",
    "SSIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23971,), 23971, (7991,), 7991)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stemmed\n",
    "(X_train_stemmed, y_train_stemmed), (X_test_stemmed, y_test_stemmed) = (combine_df.tweet_stemmed.apply(lambda x: ' '.join(x)).values[:SSIZE], combine_df.label.tolist()[:SSIZE]), (combine_df.tweet_stemmed.apply(lambda x: ' '.join(x)).values[SSIZE:], combine_df.label.tolist()[SSIZE:])\n",
    "\n",
    "X_train_stemmed.shape, len(y_train_stemmed), X_test_stemmed.shape, len(y_test_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ф-ция тренировки модели\n",
    "#predict_proba, если предсказание >= 0.3 то 1, иначе 0\n",
    "\n",
    "def train_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict_proba(X_test)\n",
    "    return f1_score(y_test, preds[:,1]>0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5579937304075234"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Рассчитаем f1_score для CountVectorizer\n",
    "count_vec = CountVectorizer(max_df=0.9, stop_words='english', max_features=1000)\n",
    "xtrain_stemmed_count_vec = count_vec.fit_transform(X_train_stemmed)\n",
    "xtest_stemmed_count_vec = count_vec.transform(X_test_stemmed)\n",
    "\n",
    "clf = linear_model.LogisticRegression()\n",
    "f1_score_stemmed_count_vec = train_model(\n",
    "    clf, \n",
    "    xtrain_stemmed_count_vec, \n",
    "    y_train_stemmed, \n",
    "    xtest_stemmed_count_vec, \n",
    "    y_test_stemmed)\n",
    "f1_score_stemmed_count_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5579937304075234"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Рассчитаем f1_score для TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer(max_df=0.9, stop_words='english', max_features=5000)\n",
    "xtrain_stemmed_tfidf_vec = count_vec.fit_transform(X_train_stemmed)\n",
    "xtest_stemmed_tfidf_vec = count_vec.transform(X_test_stemmed)\n",
    "\n",
    "clf = linear_model.LogisticRegression()\n",
    "f1_score_stemmed_tfidf_vec = train_model(\n",
    "    clf, \n",
    "    xtrain_stemmed_tfidf_vec, \n",
    "    y_train_stemmed, \n",
    "    xtest_stemmed_tfidf_vec, \n",
    "    y_test_stemmed)\n",
    "f1_score_stemmed_tfidf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatized\n",
    "(X_train_lemmatized, y_train_lemmatized), (X_test_lemmatized, y_test_lemmatized) = (\n",
    "    combine_df.tweet_lemmatized.apply(lambda x: ' '.join(x)).values[:SSIZE], combine_df.label.tolist()[:SSIZE]\n",
    "), (\n",
    "    combine_df.tweet_lemmatized.apply(lambda x: ' '.join(x)).values[SSIZE:], combine_df.label.tolist()[SSIZE:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5473908413205538"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Рассчитаем f1_score для CountVectorizer\n",
    "\n",
    "count_vec = CountVectorizer(max_df=0.9, stop_words='english', max_features=1000)\n",
    "\n",
    "xtrain_lemmatized_count_vec = count_vec.fit_transform(X_train_lemmatized)\n",
    "xtest_lemmatized_count_vec = count_vec.transform(X_test_lemmatized)\n",
    "\n",
    "clf = linear_model.LogisticRegression()\n",
    "\n",
    "f1_score_lemmatized_count_vec = train_model(\n",
    "    clf, \n",
    "    xtrain_lemmatized_count_vec, \n",
    "    y_train_lemmatized, \n",
    "    xtest_lemmatized_count_vec, \n",
    "    y_test_lemmatized)\n",
    "f1_score_lemmatized_count_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5497326203208557"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Рассчитаем f1_score для TfidfVectorizer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(max_df=0.9, stop_words='english', max_features=1000)\n",
    "xtrain_lemmatized_tfidf_vec = tfidf_vec.fit_transform(X_train_lemmatized)\n",
    "xtest_lemmatized_tfidf_vec = tfidf_vec.transform(X_test_lemmatized)\n",
    "\n",
    "clf = linear_model.LogisticRegression()\n",
    "\n",
    "f1_score_lemmatized_tfidf_vec = train_model(\n",
    "    clf, \n",
    "    xtrain_lemmatized_tfidf_vec, \n",
    "    y_train_lemmatized, \n",
    "    xtest_lemmatized_tfidf_vec, \n",
    "    y_test_lemmatized)\n",
    "f1_score_lemmatized_tfidf_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 2.\n",
    "Построим модель LogisticRegression, используя TF-IDF признаки для столбца combine_df['tweet_stemmed']. \n",
    "- Поделим TF-IDF признаки на train, test (train заканчивается на 31962 строке combine_df)\n",
    "- Ответами является столбец train_df['label']\n",
    "- Рассчитаем predict_proba, приведем prediction в в бинарный вид: если предсказание >= 0.3 то 1, иначе 0, тип заменим на int\n",
    "- Рассчитаем f1_score \n",
    "\n",
    "Повторим аналогично для столбца combine_df['tweet_lemmatized'].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3.\n",
    "Выведите результаты f1-score всех моделей, сделайте вывод.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5579937304075234"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_stemmed_count_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5579937304075234"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_stemmed_tfidf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5473908413205538"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_lemmatized_count_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5497326203208557"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_lemmatized_tfidf_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вывод:\n",
    "Стемминг показал лучший результат. CountVectorizer и TfidfVectorizer показали одинаковый результат"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 4.\n",
    "Теперь перейдем к визуализации. Посмотрим, какие слова являются наиболее популярные в датасете с помощью облака слов (WordCloud).\n",
    "Облако слов - это визуализация, в которой наиболее частые слова большого размера, а менее частые слова меньшего размера.\n",
    "- объединим слова в одну строку\n",
    "- создадим словарь частот слов с помощью collections.Counter\n",
    "- нарисуем облако слов с частотами слов с помощью WordCloud.generate_from_frequencies()\n",
    "- используем nltk.corpus.stopwords как параметр stopwords, чтобы убрать \"мусорные\" частотные слова\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### На 3.8 pithon не смогла установить WordCloud без ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 5.\n",
    "Теперь отобразим облако слов для отзывов, не содержащих токсичных комментариев (combine_df['label'] == 0). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 6.\n",
    "Теперь отобразим облако слов для отзывов, содержащих токсичные комментарии (combine_df['label'] == 1). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
