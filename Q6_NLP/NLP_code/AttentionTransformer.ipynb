{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# В предыдущей серии\n",
    "\n",
    "\n",
    "<img src=\"images/RNNCompar.png\"/>\n",
    "\n",
    "\n",
    "Мы посмотрели на задачу классификации текстов. Но есть ряд более сильных подходов, которые лучше показывать через задачу генерации\n",
    "\n",
    "\n",
    "# Генерация текстов, encoder-decoder\n",
    "\n",
    "<img src=\"images/EncDec.png\"/>\n",
    "\n",
    "\n",
    "Данная архитектура называется seq2seq, простыми словами выглядит она следующим образом:\n",
    "<img src=\"images/seq2seq.png\"/>\n",
    "\n",
    "\n",
    "эту модель можно строить на уровне слов и на уровне токенов. Попробуем обучить на уровне токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 30\n",
    "latent_dim = 256\n",
    "num_samples = 10000\n",
    "data_path = 'data/fra-eng/fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем из текстов токены и делаем pne-hot вектора на каждый токен\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    \n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 1.3435 - accuracy: 0.7200 - val_loss: 1.0887 - val_accuracy: 0.7001\n",
      "Epoch 2/30\n",
      "125/125 [==============================] - 22s 175ms/step - loss: 0.8934 - accuracy: 0.7538 - val_loss: 0.9304 - val_accuracy: 0.7460\n",
      "Epoch 3/30\n",
      "125/125 [==============================] - 20s 163ms/step - loss: 0.7628 - accuracy: 0.7930 - val_loss: 0.7988 - val_accuracy: 0.7777\n",
      "Epoch 4/30\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.6550 - accuracy: 0.8158 - val_loss: 0.7049 - val_accuracy: 0.7972\n",
      "Epoch 5/30\n",
      "125/125 [==============================] - 20s 161ms/step - loss: 0.5925 - accuracy: 0.8307 - val_loss: 0.6565 - val_accuracy: 0.8107\n",
      "Epoch 6/30\n",
      "125/125 [==============================] - 23s 181ms/step - loss: 0.5537 - accuracy: 0.8407 - val_loss: 0.6258 - val_accuracy: 0.8176\n",
      "Epoch 7/30\n",
      "125/125 [==============================] - 29s 230ms/step - loss: 0.5255 - accuracy: 0.8473 - val_loss: 0.6020 - val_accuracy: 0.8236\n",
      "Epoch 8/30\n",
      "125/125 [==============================] - 42s 333ms/step - loss: 0.5043 - accuracy: 0.8527 - val_loss: 0.5850 - val_accuracy: 0.8290\n",
      "Epoch 9/30\n",
      "125/125 [==============================] - 38s 305ms/step - loss: 0.4858 - accuracy: 0.8579 - val_loss: 0.5659 - val_accuracy: 0.8356\n",
      "Epoch 10/30\n",
      "125/125 [==============================] - 38s 303ms/step - loss: 0.4706 - accuracy: 0.8620 - val_loss: 0.5497 - val_accuracy: 0.8396\n",
      "Epoch 11/30\n",
      "125/125 [==============================] - 38s 305ms/step - loss: 0.4562 - accuracy: 0.8662 - val_loss: 0.5392 - val_accuracy: 0.8423\n",
      "Epoch 12/30\n",
      "125/125 [==============================] - 40s 318ms/step - loss: 0.4435 - accuracy: 0.8695 - val_loss: 0.5325 - val_accuracy: 0.8430\n",
      "Epoch 13/30\n",
      "125/125 [==============================] - 40s 320ms/step - loss: 0.4321 - accuracy: 0.8725 - val_loss: 0.5211 - val_accuracy: 0.8454\n",
      "Epoch 14/30\n",
      "125/125 [==============================] - 38s 302ms/step - loss: 0.4198 - accuracy: 0.8758 - val_loss: 0.5128 - val_accuracy: 0.8487\n",
      "Epoch 15/30\n",
      "125/125 [==============================] - 38s 305ms/step - loss: 0.4102 - accuracy: 0.8780 - val_loss: 0.5051 - val_accuracy: 0.8509\n",
      "Epoch 16/30\n",
      "125/125 [==============================] - 39s 315ms/step - loss: 0.3993 - accuracy: 0.8813 - val_loss: 0.4989 - val_accuracy: 0.8522\n",
      "Epoch 17/30\n",
      "125/125 [==============================] - 37s 299ms/step - loss: 0.3892 - accuracy: 0.8838 - val_loss: 0.4925 - val_accuracy: 0.8551\n",
      "Epoch 18/30\n",
      "125/125 [==============================] - 37s 297ms/step - loss: 0.3793 - accuracy: 0.8869 - val_loss: 0.4939 - val_accuracy: 0.8549\n",
      "Epoch 19/30\n",
      "125/125 [==============================] - 37s 300ms/step - loss: 0.3703 - accuracy: 0.8895 - val_loss: 0.4829 - val_accuracy: 0.8574\n",
      "Epoch 20/30\n",
      "125/125 [==============================] - 37s 299ms/step - loss: 0.3613 - accuracy: 0.8920 - val_loss: 0.4805 - val_accuracy: 0.8583\n",
      "Epoch 21/30\n",
      "125/125 [==============================] - 36s 289ms/step - loss: 0.3532 - accuracy: 0.8943 - val_loss: 0.4705 - val_accuracy: 0.8613\n",
      "Epoch 22/30\n",
      "125/125 [==============================] - 35s 281ms/step - loss: 0.3425 - accuracy: 0.8973 - val_loss: 0.4653 - val_accuracy: 0.8625\n",
      "Epoch 23/30\n",
      "125/125 [==============================] - 38s 302ms/step - loss: 0.3343 - accuracy: 0.8997 - val_loss: 0.4617 - val_accuracy: 0.8644\n",
      "Epoch 24/30\n",
      "125/125 [==============================] - 38s 306ms/step - loss: 0.3256 - accuracy: 0.9024 - val_loss: 0.4634 - val_accuracy: 0.8641\n",
      "Epoch 25/30\n",
      "125/125 [==============================] - 38s 303ms/step - loss: 0.3179 - accuracy: 0.9043 - val_loss: 0.4547 - val_accuracy: 0.8672\n",
      "Epoch 26/30\n",
      "125/125 [==============================] - 38s 305ms/step - loss: 0.3079 - accuracy: 0.9077 - val_loss: 0.4538 - val_accuracy: 0.8666\n",
      "Epoch 27/30\n",
      "125/125 [==============================] - 32s 260ms/step - loss: 0.3005 - accuracy: 0.9097 - val_loss: 0.4514 - val_accuracy: 0.8675\n",
      "Epoch 28/30\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.2931 - accuracy: 0.9117 - val_loss: 0.4494 - val_accuracy: 0.8687\n",
      "Epoch 29/30\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.2839 - accuracy: 0.9146 - val_loss: 0.4495 - val_accuracy: 0.8691\n",
      "Epoch 30/30\n",
      "125/125 [==============================] - 20s 163ms/step - loss: 0.2765 - accuracy: 0.9166 - val_loss: 0.4486 - val_accuracy: 0.8701\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Vas-me !\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: C'est maintenant.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: C'est maintenant.\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Coure !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Coure !\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Qui est-il ?\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: À la somme !\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Parse-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: À le faites !\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Jouvez-moi.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrêtez-vois !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrêtez-vois !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrêtez-vois !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Allez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Allez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Allez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Saute un !\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Saute un !\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je suis en train de partir.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Je suis en train de partir.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je veux !\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je veux !\n",
      "\n",
      "-\n",
      "Input sentence: I won.\n",
      "Decoded sentence: Je suis en train de partir.\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Je vous partir !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attaque !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attaque !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Sarre !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Sarre !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Sarre !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Sarre !\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Sorrez !\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Allez au souveau !\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Allez au souveau !\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Allez au souveau !\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Compes !\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Compes !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Compris ?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Compris ?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Compris ?\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Inontez aven !\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Inontez aven !\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Serrez-vous sour !\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Serrez-vous sour !\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je suis sentie.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je suis sentie.\n",
      "\n",
      "-\n",
      "Input sentence: I know.\n",
      "Decoded sentence: Je sais.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je suis remande.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je suis remande.\n",
      "\n",
      "-\n",
      "Input sentence: I lied.\n",
      "Decoded sentence: J'ai parti.\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: Je suis ma partir.\n",
      "\n",
      "-\n",
      "Input sentence: I paid.\n",
      "Decoded sentence: J'araive.\n",
      "\n",
      "-\n",
      "Input sentence: I'm 19.\n",
      "Decoded sentence: Je suis en train de partir.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je suis en train de partir.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je suis en train de partir.\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: Écaisez-le.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Il n'u pas !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Il n'u pas !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Il n'u pas !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Il n'u pas !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Il n'u pas !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Il n'u pas !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Il n'u pas !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Il n'u pas !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Il n'u pas !\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Vraiment ?\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Vraiment ?\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Vraiment ?\n",
      "\n",
      "-\n",
      "Input sentence: Thanks.\n",
      "Decoded sentence: Prenez !\n",
      "\n",
      "-\n",
      "Input sentence: We try.\n",
      "Decoded sentence: Nous avons seul.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous nous sommes !\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous nous sommes !\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous nous sommes !\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous nous sommes !\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: Demandez-le.\n",
      "\n",
      "-\n",
      "Input sentence: Awesome!\n",
      "Decoded sentence: Fartes moi !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Sois dentie !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Sois dentie !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Sois dentie !\n",
      "\n",
      "-\n",
      "Input sentence: Be cool.\n",
      "Decoded sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez à moi !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez à moi !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez à moi !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez à moi !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez à moi !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez à moi !\n",
      "\n",
      "-\n",
      "Input sentence: Be kind.\n",
      "Decoded sentence: Sois sentie !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Attez le !\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Appelez-moi.\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Appelez-moi.\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Appelez-moi.\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Appelez-moi.\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Entrez !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но есть проблемы:\n",
    "- на длинных последовательностях результат будет не очень - быстро забывается контекст\n",
    "- хочется научить сеть смотреть в определенное место в прошлом при генерации\n",
    "\n",
    "attention\n",
    "\n",
    "<img src=\"images/Attention.png\"/>\n",
    "\n",
    "<img src=\"images/Attention2.png\"/>\n",
    "\n",
    "\n",
    "- h(t): скрытое состояние декодера\n",
    "- c(t): вектор контекста, который подается на вход\n",
    "- y(t): текущий таргет\n",
    "- $\\bar{h}(t)$: скрытое состояние attention\n",
    "- a(t): скор нормализации\n",
    "\n",
    "\n",
    "$$\\bar{h}(t)\\ =\\ tanh(W_c\\ [c_t,\\ h_t]) $$\n",
    "\n",
    "$$P(y_t|y_{<t},\\ x)\\ =\\ softmax(W_s\\ \\bar{h}_t) $$\n",
    "\n",
    "\n",
    "Зачем нужен скор нормализации? - пытаемся сравнить похожесть текущего скрытого состояния и скрытого состояния из прошлого и понять, на что обращать внимание\n",
    "\n",
    "\n",
    "$$a_t(s)\\ =\\ \\frac{exp(score(h_t,\\ \\bar{h}_s))}{\\sum_{i}\\ exp(score(h_t,\\ \\bar{h}_i)) }$$ \n",
    "\n",
    "\n",
    "<img src=\"images/scores.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поднимемся на уровень слов, чтобы можно было что-то более адекватное посчитать за адекватное время"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow.compat.v1 as tf\n",
    "data_path = 'data/fra-eng/fra.txt'\n",
    "num_samples = 10000\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(preprocess_sentence(input_text))\n",
    "    target_texts.append(preprocess_sentence(target_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, inp_lang_tokenizer = tokenize(input_texts)\n",
    "target_tensor, targ_lang_tokenizer = tokenize(target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.lstm(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "    \n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.lstm(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 1.6342\n",
      "Epoch 2 Loss 1.2145\n",
      "Epoch 3 Loss 1.0019\n",
      "Epoch 4 Loss 0.8562\n",
      "Epoch 5 Loss 0.7352\n",
      "Epoch 6 Loss 0.6251\n",
      "Epoch 7 Loss 0.5332\n",
      "Epoch 8 Loss 0.4489\n",
      "Epoch 9 Loss 0.3762\n",
      "Epoch 10 Loss 0.3177\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые украденные функции для оценки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "\n",
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    fontdict = {'fontsize': 14}\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "    #ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    #ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.show()\n",
    "    \n",
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Input: <start> good morning <end>\n",
      "Predicted translation: bonjour ! <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-ccddba35bea7>:38: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
      "<ipython-input-14-ccddba35bea7>:39: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmoAAAJyCAYAAAB0V+yJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAfbElEQVR4nO3dd7SlB1nv8d9DKiFEeiiXjkhVSiBgRJFwBVERFRtFmgSVYkO96L1SrCAoUXRRBKTJBRUMXF0oBBQMEJooASSAFGkCAoYEUoDn/rH3hDM7M5OZkMz7HObzWWtWznnffc5+ZtbO2d/z1uruAAAwz6WWHgAAgF0TagAAQwk1AIChhBoAwFBCDQBgKKEGADCUUAMAGEqoAQAMJdQAAIYSagAAQwk1AC4WVfWVqvrybv6cVVX/UlWPWHpO2E4OXnoAAL5uPCzJY5K8NMmp62XHJrlHkscnuWaS362q7u4/WmJA2G7KTdnZlar6xiRPS/Kz3f2OpecB5quqk5K8rLufubH8QUnu3t3fX1U/leTh3X3TRYaEbcauT3bnfknumOSBC88BbB/HJ/nHXSz/xyR3Xn/8yiTX3W8TwTYn1LiAqqok903yrCT3qqqDFh4J2B7+K6vdnJvukeTT64+PTPLf+2ke2PYco8au3DHJZZM8Isl3J7lbkpcvORCwLTw2yTOq6k5J3rRedpsk35XkwevP/2d2vdUN2AXHqHEBVfVnSc7t7hOq6klJrt3d91x4LGAbqKrbJ3l4khutF/1bkj/s7jcuNxVsX0KNnVTVZZJ8PMn3dPfrquoWSd6Q5Grd/bklZwOAA41dn2z6oSSf7u7XJUl3v72q3pvkx5I8ddHJgG2hqq6e5CrZOA66u9+2zERMst4g8ENJTupuxyteCCcTsOm+SZ6/sez5Se6//0cBtpOqumVVvTPJfyR5W5K3bPnz5iVnY5QfSfLsrN5vuBB2fXK+qrpmkg8kuXF3v3fL8v+R5INJbtLdpy80HjBcVb05qzM/H5fkY0l2eoPp7g8tMRezVNVrkhyd5AvdfczS80wn1AC4WFTVWUlu6Rc6dqeqrpPk9CS3TfLGJLfq7nctOtRwdn2yk6q61vo6artct7/nAbaVdyS56tJDMNp9k7yuu9+e5G+zurg6eyDU2PSBJFfeXFhVV1yvA9idX03yhKq6c1UdXVVX2Ppn6eEY4SeSPG/98QuS3Ht3GwdYseuTnVTVV5Ic3d2f2lh+7STv6u7LLDMZMN3658cOW99cKkl3t7ucHMCq6luT/H2Sq3b3mVV1aJJPJPnR7n7lstPN5fIcJEmq6g/XH3aS36mqL2xZfVBWxxO8fX/PBWwr37n0AIx2v6wuyXFmknT3uVX14qyuKiDUdkOoscPN1/+tJDdOcu6Wdedmdar9E/f3UMD20d1uDcUuVdVhWV2W48c3Vj0/yd9V1ZE7Ao6d2fXJ+dbHCbw4yQO7+/NLzwPMV1W3SvL27v7K+uPdcsHbA1dVXSmr+0Y/v7u/srHuPkle1d2fWGS44YQa56uqg5KcneRbnC4N7I31cWlX7e5Prj/urLbMb3KMGlwEdn1yvu7+clV9KMmhS88CbBvXTfKpLR8DFyNb1NhJVd0vq2MI7tPdn156HgC2r6r6QDbuULE73X29S3icbckWNTY9Mqvfij9aVR9JctbWld39zYtMBWwLVXVEkltk1zdlf8kSM7Gop2z5+Mgkv5DkTUnesF52+6yuKvCk/TzXtiHU2PSXSw/AHFX163v72O5+3CU5C/NV1Z2TvDDJFXexurO61A8HkO4+P8Cq6s+SPL67f3vrY6rqUUluup9H2zbs+gR2q6resbHo2kmOyOqG20ly9SRfSPJBW1upqncmeXOSX+3uj13Y4zmwVNUZWd3b830by2+Q5G3dfdQyk81mixqwW9294/p6qaoHZHX7l/t194fXy66V5NlZ3QoGrpPk7iKN3TgryR2TvG9j+R2z+oWPXRBq7GR9S49fy+qEgmslOWTreqfXH9B+Pck9dkRaknT3h6vqF5OclORZi03GFKck+aYk7196EEb6gyR/XFXHJHnjetntsrpjwWOWGmo6ocam30jyo0l+J6v/qX4pq9+SfyzJ/1luLAY4Osmld7H88CRX2s+zMNNTkzyxqq6e5B1Jztu60gVvD2zd/YSq+mCSn83qLgVJ8u6sttK/eLHBhnOMGjtZn0r90939iqr6fJJbdPf7q+qnkxzf3fdceEQWUlUnJblekgdndRxSZ3W21tOSfKC777HcdEywcVP2TS54CxeBLWpsOjrJjrsSnJnkcuuPX5Hk8UsMxBg/meQ5SV6f5MvrZZdK8ndZxRu44C17paoulwtevuUzy0wzm1Bj04ezOpPvw1kd8HmXJG/N6lo3X1xwLhbW3Z9KcrequmGSG60X/1t3n77gWAxRVYckOTWrLe/vXHoe5qmqa2e1e/yO2fkOOBWXb9ktocamlyY5PqsDPU9M8sKqenCSayT5vSUHY4buPr2qPrb6sM+60C/ggNDd51XVednLq9BzQHp2VntpHpTVJX68VvaCY9TYo6o6NslxSU7v7v+39Dwsq6oemuRXsgr3JPlIVhew/JPlpmKKqvrlJDdP8oDu/tLS8zBLVZ2Z5HbdfdrSs2wntqixk6r69iSv3/FDtrtPTXJqVR1cVd/e3a9ddkKWUlW/muRRSZ6Y5J/Wi++Q5Her6qju/t3FhmOKOyT5jqxuQXdaLngLursvMhVTfCDJYUsPsd3YosZOqurLSa7W3Z/cWH7FJJ901taBq6o+nORXuvuFG8vvneS3u/vay0zGFFX17D2t7+4H7K9ZmKeq7pTkfyX5mc27E7B7Qo2drE+vP3p94PjW5TdM8ha3+DhwVdXZSW62i9u/fGOSd3T34ctMBmwH60s+HZbVSQPnJNlp97j3l12z65MkSVW9bP1hJ3l+VZ2zZfVBSW6W1WUZOHCdnuReSTZvvn6vJO/Z/+MwVVVdL8lNsvp58u7u/veFR2KGhy09wHYk1Njhv9b/rSSfzc6X4jg3q2OSnrG/h2KUxyR58fo4xlPWy47L6pikH15qKOaoqqOSPDPJDyX5ylcX118leVB3f36x4Vhcdz9n6Rm2I7s+2UlVPTrJE112gV2pqlsn+fkkN14veneSJ3X3Py83FVOsj1H71iQn5Ktb4I/L6tpZp3T3g5aajRmq6ugk901y/ST/p7s/XVXHJflYd39g2elmEmrspKoulSTd/ZX151dN8r1J3tXddn0Cu1VV/5XkHt39uo3l357kpd19xWUmY4L1L3onZ3X2502T3Ki7/72qHpPkht19ryXnm8quTzb9TVa3izqxqo5M8pYkl0lyZFU9qLufu+h0LKqqDkty73z1+KN3Jnlhd5+zxy/kQHHpfPUwiq0+k8TJJjwxyYnd/ej1iQU7/F0SZwTvxqUu/CEcYI5J8ur1xz+Y5IwkV8nqXo6PXGoolldVN0ny3iS/n+TYJLdL8uQkp1fVjffwpRw4TknyG1V1xI4FVXWZJI+Nk5FIbp3V/YI3fTyr+0yzC7aosenIJJ9bf/xdWe2uOK+qXp3kjxebiglOTPLPSe7b3Wck5x88/vysgu0uy43GED+f1daRj1bVv66X3Tyrk5O+a7GpmOKLSS6/i+U3SvLJXSwnQo0L+nCS46rq5Vm98e44m+8KSb6w2FRMcFyS2+yItCTp7jOq6teyujcsB7juPm19Xb175asnnDwvyQu6+4u7/0oOECcleXRV7Xhf6aq6TpLHJ/mrxaYazq5PNv1+Vj9YP5Lko0l23DLq25O8Y6mhGOHsrG6ovOkb1usgSS6b1TFp703y/iSHJnlAVf3MolMxwSOz+qX/U0mOyOqyT+9L8t9J/veCc43mrE8uYH1mzrWSvLK7z1wv+54kn+vuU/b4xXzdqqrnJLlNVscr7tiCdvskT0vyJrcHoqruk+RP89XrMW59g+nuvvoigzHK+lZSt8pqY9HbuvtVC480mlDjfFX1DUm+efPU+vW647K6RMdn9/9kTFBVl8vqQODvS/Ll9eKDstqd8YDu/twykzFFVX0oq9fI47r7Sxf2eA4c3l8uOqHG+arqslmdfXOXrVvOqupbkrwpyTW6+9NLzccMVXWDbLngrZsrs0NVfTbJrd0yik3eXy46ocZOquoFSc7s7odsWfbErC5GePflJmNpVfWs3azqrI5Re1+SF3X3x/bfVExSVU9J8p7u/qOlZ2Ee7y8XjVBjJ1V1lyQvTHLV7j53faeCjyR5WHe/ZNnpWNL6TOA7ZHUPx9PWi2+W1fFIb83qSuNHJrlDd799iRlZVlUdmuSvs7o/8DuSnLd1fXc/boGxGML7y0Xj8hxsemVW17r53iQvSXJ8VmdtvXzJoRjhlCRnZnVz7S8kyfrCps9I8i9J7pbkuUmelNXrhgPPQ5LcNcmnk9wgGycTJBFqBzbvLxeBLWpcQFU9Psk3dfc9quq5ST7f3Q9dei6WVVUfT3Kn7n73xvKbJDm5u69WVbdM8ir3dDwwVdUnk/xOd//B0rMwk/eXfWeLGrvy3CRvraprJfmB2DrCypFJrpbk3RvLr7pel6xuOebnyoHroCQvW3oIRvP+so9c8JYL6O53ZnUM0guSfKS737TwSMzw0iTPrKofrqrrrP/8cJJnZrUbI0lum+T0xSZkac9Ocu+lh2Au7y/7zm++7M5zs7p/468tPAdz/FRWd654fr76s+NLSZ6V1RXHk9XWtgfv/9EY4ogkP7k+aPxfc8GTCR6xyFRM4/1lHzhGjV2qqiskeXiSp3X3J5aehzmq6jJJrr/+9P3dfdaS8zBHVb1mD6u7u++034ZhLO8v+0aoAQAM5Rg1AIChhBoAwFBCjT2qqhOWnoG5vD7YHa8N9sTrY+8JNS6M/5nYE68Pdsdrgz3x+thLQg0AYChnfW44tA7rw3OZpccY47yck0Ny2NJjMJTXB7vjtcGeeH3s7OyclXP7nNrVOhe83XB4LpNjyx0tAID949Q+ebfr7PoEABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoS401KrqH6rqKZfkEFV1/6o685J8DgCA7WbKFrUXJbne0kMAAExy8NIDJEl3fzHJFy/p56mqQ7v73Ev6eQAALg57u0Xt4Ko6sao+u/7ze1V1qSSpqstX1XPWy79YVa+qqpvu+MIduzWr6viqOq2qzqqq11TVdTcfs/UJq+ohVfW+qjp3/d8Hb6zvqrrnxrIPVtUjNx7z0Kp6SVWdleS39/6fBgBgWXsbavdeP/b2SR6S5IQkP7de92dJjk3y/Ulum+QLSV5RVZfe8vWHJXlUkgeuv8flkjx1d09WVT+Q5ClJnpzkZklOTPInVfV9eznvVo9O8rdJbp7kjy/C1wMALGJvd31+PMkjuruT/FtV3TDJL1TVy5PcPcl3dPdrk6Sq7pvkw1nF3Z9ueZ6Hdvd71o95YpJnVVWtv+emRyZ5XnfvOInh9Kq6dZJfSfLyffw7vqi7/3RPD6iqE7KKzxyeI/bx2wMAXDL2dovaGzeC6g1JrpHkxkm+sv48SdLd/53kHUlusuXx5+yItLWPJTk0yeV383w3TnLKxrJ/2viee+stF/aA7n56dx/T3cccksMuwlMAAFz8LsmzPreG3Zd2s25fn783Pq6N9Yfs4mvO2sfnAAAYYW9D6diq2hpFt8tqq9i789Vj15IkVXVUVseDvetrmOvdSY7bWPZtG9/zU0mutuV5j976OQDAdre3x6hdPcmTq+pPsoqwX0rym9393qo6KcnT1sd5fS7JbyU5I8mffw1z/V6Sv6iqtyb5+yR3zeqYtx/c8phXJ3loVb0+yZezOqPz7K/hOQEARtnbUHtBkoOSnJrVLsdnJvmD9boHZHV25suSHJ7VsWV3XV8b7SLp7r+uqodndVLBk5N8KMnPdPfWEwl+cT3HPyT5zyS/nNWxbQAAXxdq1ydd7uchqh6S5LHdfdWlZzmqrtDH1vFLjwEAHCBO7ZNzRn9m87j7JANuIVVV10xytySnLT0LAMAkE24h9bYkH01y/4XnAAAYZfFQ6+4rLz0DAMBEi+/6BABg14QaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoQ5eeoBp6pBDcvDRV196DIY671pXXnoEBjv4jLOXHoHB/vaVL1p6BIa67V2+sNt1tqgBAAwl1AAAhhJqAABDCTUAgKGEGgDAUEINAGAooQYAMJRQAwAYSqgBAAwl1AAAhhJqAABDCTUAgKGEGgDAUEINAGAooQYAMJRQAwAYSqgBAAwl1AAAhhJqAABDCTUAgKGEGgDAUEINAGAooQYAMJRQAwAYSqgBAAwl1AAAhhJqAABDCTUAgKGEGgDAUEINAGAooQYAMJRQAwAYSqgBAAwl1AAAhhJqAABDCTUAgKGEGgDAUEINAGAooQYAMJRQAwAYSqgBAAwl1AAAhhJqAABDCTUAgKGEGgDAUEINAGAooQYAMJRQAwAYSqgBAAwl1AAAhhJqAABDCTUAgKGEGgDAUEINAGAooQYAMJRQAwAYSqgBAAwl1AAAhhJqAABDCTUAgKGEGgDAUEINAGAooQYAMJRQAwAY6oAItao6raoes/QcAAD74oAINQCA7UioAQAMJdQAAIY6eOkBJqiqE5KckCSHH3TZhacBAFixRS1Jdz+9u4/p7mMOvdSllx4HACCJUAMAGOuA2PXZ3TdbegYAgH11QGxRq6qTq+phS88BALAvDohQS3L9JFdaeggAgH1xoOz6vM7SMwAA7KsDZYsaAMC2I9QAAIYSagAAQwk1AIChhBoAwFBCDQBgKKEGADCUUAMAGEqoAQAMJdQAAIYSagAAQwk1AIChhBoAwFBCDQBgKKEGADCUUAMAGEqoAQAMJdQAAIYSagAAQwk1AIChhBoAwFBCDQBgKKEGADCUUAMAGEqoAQAMJdQAAIYSagAAQwk1AIChhBoAwFBCDQBgKKEGADCUUAMAGEqoAQAMJdQAAIYSagAAQwk1AIChhBoAwFBCDQBgKKEGADCUUAMAGEqoAQAMJdQAAIYSagAAQwk1AIChhBoAwFBCDQBgKKEGADCUUAMAGEqoAQAMJdQAAIYSagAAQwk1AIChhBoAwFBCDQBgKKEGADCUUAMAGEqoAQAMJdQAAIYSagAAQwk1AIChqruXnmGUo+oKfWwdv/QYwHZUtfQETOb9lt04tU/OGf2ZXf4AsUUNAGAooQYAMJRQAwAYSqgBAAwl1AAAhhJqAABDCTUAgKGEGgDAUEINAGAooQYAMJRQAwAYSqgBAAwl1AAAhhJqAABDCTUAgKGEGgDAUEINAGAooQYAMJRQAwAYSqgBAAwl1AAAhhJqAABDCTUAgKGEGgDAUEINAGAooQYAMJRQAwAYSqgBAAwl1AAAhhJqAABDCTUAgKGEGgDAUEINAGAooQYAMJRQAwAYSqgBAAwl1AAAhhJqAABDCTUAgKGEGgDAUEINAGAooQYAMJRQAwAYSqgBAAwl1AAAhhJqAABDCTUAgKGEGgDAUEINAGAooQYAMJRQAwAYSqgBAAwl1AAAhhJqAABDCTUAgKGEGgDAUEINAGAooQYAMJRQAwAYSqgBAAwl1AAAhhJqAABDCTUAgKG2ZahV1SOr6oNLzwEAcEnalqEGAHAguNhDraqOqqrLXdzf90Ke88pVdfj+fE4AgEvaxRJqVXVQVd2lqv48ySeSfMt6+TdU1dOr6pNV9fmq+seqOmbL192/qs6squOr6rSqOquqXlNV1934/r9cVZ9YP/a5SY7cGOFuST6xfq7jLo6/EwDA0r6mUKuqm1bVE5L8R5IXJTkryV2TvLaqKsnfJLlGku9Ncsskr03y6qq62pZvc1iSRyV5YJLbJ7lckqdueY4fSfKbSR6d5FZJ3pPkFzZGeUGSeyW5bJJXVtX7qurXN4NvD3+PE6rqLVX1lvNyzj78CwAAXHKqu/ftC6qumOTeSe6X5OZJXpHkeUle3t1nb3ncnZK8LMmVu/uLW5a/Pcmfd/cTqur+SZ6d5Ebd/Z71+nsneVaSw7u7q+r1Sd7Z3Q/e8j1eleQG3X2dXcx3VJJ7Jrlvkjsk+ackz03y4u4+88L+fkfVFfrYOn7v/0EAdqhaegIm28f3Ww4cp/bJOaM/s8sfIBdli9rDk5yY5OwkN+zuu3f3X2yNtLVbJzkiyafWuyzPrKozk9wsyfW3PO6cHZG29rEkhya5/PrzGyd5w8b33vz8fN19Rnc/q7u/M8ltkhyd5JlZxRsAwLZx8EX4mqcnOS/JTyQ5rapemtUWtZO7+8tbHnepJP+Z1VatTWds+fhLG+t2/MpxkXbLVtVhWe1qvU9Wx669M8nPJTnponw/AICl7HMMdffHuvu3uvubktw5yZlJ/m+Sj1TVk6rqFuuHvi2rrVlf6e73bfz55D485buT3G5j2U6f18q3VdXTsjqZ4Y+SvC/Jrbv7Vt19Ynd/dl//rgAAS/qaTibo7jd2908nuVpWu0RvmOTNVXWHJK9KckqSk6rqu6vqulV1+6p67Hr93joxyf2q6sFV9Y1V9agkx2485j5J/j7JUUl+PMk1u/uXuvu0r+XvBwCwpIuy6/MCuvucJH+Z5C+r6ipJvrw+EeBuWZ2x+YwkV8lqV+gpWR3cv7ff+0VVdb0kv5XVMW8vS/L7Se6/5WEnJ7lqd59xwe8AALA97fNZn1/vnPUJXGTO+mRPvN+yGxf3WZ8AAOwHQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABjq4KUHAPi60b30BMDXGVvUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoYQaAMBQQg0AYCihBgAwlFADABhKqAEADCXUAACGEmoAAEMJNQCAoQ5eeoAJquqEJCckyeE5YuFpAABWbFFL0t1P7+5juvuYQ3LY0uMAACQRagAAYwk1AIChhBoAwFBCDQBgKKEGADCUUAMAGEqoAQAMJdQAAIYSagAAQwk1AIChhBoAwFBCDQBgKKEGADCUUAMAGEqoAQAMJdQAAIYSagAAQwk1AIChhBoAwFBCDQBgKKEGADCUUAMAGEqoAQAMJdQAAIYSagAAQwk1AIChhBoAwFBCDQBgKKEGADCUUAMAGEqoAQAMJdQAAIYSagAAQwk1AIChhBoAwFBCDQBgKKEGADCUUAMAGEqoAQAMJdQAAIYSagAAQwk1AIChhBoAwFBCDQBgKKEGADCUUAMAGEqoAQAMJdQAAIYSagAAQwk1AIChhBoAwFBCDQBgKKEGADCUUAMAGEqoAQAMJdQAAIYSagAAQwk1AIChhBoAwFBCDQBgKKEGADCUUAMAGEqoAQAMJdQAAIYSagAAQwk1AIChqruXnmGUqvpUkg8tPccgV0ry6aWHYCyvD3bHa4M98frY2bW7+8q7WiHU2KOqekt3H7P0HMzk9cHueG2wJ14fe8+uTwCAoYQaAMBQQo0L8/SlB2A0rw92x2uDPfH62EuOUQMAGMoWNQCAoYQaAMBQQg0AYCihBgAwlFADABjq/wPt/nJ0ERMutQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "translate(u'good morning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "<img src=\"images/transformer.png\"/>\n",
    "\n",
    "\n",
    "Идея в том, что каждое слово параллельно проходит через слои, изображенные на картинке.\n",
    "Некоторые из них — это стандартные fully-connected layers, некоторые — shortcut connections как в ResNet (там, где на картинке Add).\n",
    "\n",
    "\n",
    "Multi-head attention - это специальный новый слой, который дает возможность каждому входному вектору взаимодействовать с другими словами через attention mechanism, вместо передачи hidden state как в RNN или соседних слов как в CNN.\n",
    "\n",
    "\n",
    "<img src=\"images/mha.png\"/>\n",
    "\n",
    "\n",
    "<img src=\"images/AttTr.png\"/>\n",
    "\n",
    "\n",
    "Работа энкодера:\n",
    "\n",
    "\n",
    "Делаются эмбеддинги для всех слов предложения (вектора одинаковой размерности). Для примера пусть это будет предложение I am stupid. В эмбеддинг добавляется еще позиция слова в предложении.\n",
    "\n",
    "\n",
    "Берется вектор первого слова и вектор второго слова (I, am), подаются на однослойную сеть с одним выходом, которая выдает степень их похожести (скалярная величина). Эта скалярная величина умножается на вектор второго слова, получая его некоторую \"ослабленную\" на величину похожести копию.\n",
    "\n",
    "\n",
    "Вместо второго слова подается третье слово и делается тоже самое что в п.2. с той же самой сетью с теми же весами (для векторов I, stupid).\n",
    "\n",
    "\n",
    "Делая тоже самое для всех оставшихся слов предложения получаются их \"ослабленные\" (взвешенные) копии, которые выражают степень их похожести на первое слово. Далее эти все взвешенные вектора складываются друг с другом, получая один результирующий вектор размерности одного эмбединга:\n",
    "output=am * weight(I, am) + stupid * weight(I, stupid)\n",
    "\n",
    "\n",
    "Это механизм \"обычного\" attention.\n",
    "Так как оценка похожести слов всего одним способом (по одному критерию) считается недостаточной, тоже самое (п.2-4) повторяется несколько раз с другими весами. Типа одна один attention может определять похожесть слов по смысловой нагрузке, другой по грамматической, остальные еще как-то и т.п.\n",
    "\n",
    "\n",
    "На выходе п.5. получается несколько векторов, каждый из которых является взвешенной суммой всех остальных слов предложения относительно их похожести на первое слово (I). Конкантенируем этот вректор в один.\n",
    "\n",
    "\n",
    "Дальше ставится еще один слой линейного преобразования, уменьшающий размерность результата п.6. до размерности вектора одного эмбединга. Получается некое представление первого слова предложения, составленное из взвешенных векторов всех остальных слов предложения.\n",
    "\n",
    "\n",
    "Такой же процесс производится для всех других слов в предложении.\n",
    "\n",
    "\n",
    "Так как размерность выхода та же, то можно проделать все тоже самое еще раз (п.2-8), но вместо оригинальных эмбеддингов слов взять то, что получается после прохода через этот Multi-head attention, а нейросети аттеншенов внутри взять с другими весами (веса между слоями не общие). И таких слоев можно сделать много (у гугла 6). Однако между первым и вторым слоем добавляется еще полносвязный слой и residual соединения, чтобы добавить сети выразительности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "    \n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "    \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "    \n",
    "    \n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = len(inp_lang_tokenizer.index_word) + 2\n",
    "target_vocab_size = len(targ_lang_tokenizer.index_word) + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "  \n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')\n",
    "\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "  \n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 8.2704 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 8.1042 Accuracy 0.0148\n",
      "Epoch 1 Batch 100 Loss 7.8725 Accuracy 0.0404\n",
      "Epoch 1 Loss 7.7805 Accuracy 0.0455\n",
      "Epoch 2 Batch 0 Loss 7.2858 Accuracy 0.0667\n",
      "Epoch 2 Batch 50 Loss 7.1240 Accuracy 0.0667\n",
      "Epoch 2 Batch 100 Loss 6.9004 Accuracy 0.0667\n",
      "Epoch 2 Loss 6.7806 Accuracy 0.0667\n",
      "Epoch 3 Batch 0 Loss 6.0503 Accuracy 0.0667\n",
      "Epoch 3 Batch 50 Loss 5.7819 Accuracy 0.1003\n",
      "Epoch 3 Batch 100 Loss 5.5084 Accuracy 0.1095\n",
      "Epoch 3 Loss 5.3948 Accuracy 0.1131\n",
      "Epoch 4 Batch 0 Loss 4.7114 Accuracy 0.1333\n",
      "Epoch 4 Batch 50 Loss 4.5756 Accuracy 0.1387\n",
      "Epoch 4 Batch 100 Loss 4.4015 Accuracy 0.1455\n",
      "Epoch 4 Loss 4.3359 Accuracy 0.1478\n",
      "Epoch 5 Batch 0 Loss 3.8642 Accuracy 0.1708\n",
      "Epoch 5 Batch 50 Loss 3.8154 Accuracy 0.1682\n",
      "Epoch 5 Batch 100 Loss 3.7026 Accuracy 0.1732\n",
      "Epoch 5 Loss 3.6671 Accuracy 0.1746\n",
      "Epoch 6 Batch 0 Loss 3.3714 Accuracy 0.1917\n",
      "Epoch 6 Batch 50 Loss 3.3321 Accuracy 0.1888\n",
      "Epoch 6 Batch 100 Loss 3.2596 Accuracy 0.1922\n",
      "Epoch 6 Loss 3.2400 Accuracy 0.1932\n",
      "Epoch 7 Batch 0 Loss 3.0240 Accuracy 0.2042\n",
      "Epoch 7 Batch 50 Loss 3.0168 Accuracy 0.2030\n",
      "Epoch 7 Batch 100 Loss 2.9630 Accuracy 0.2055\n",
      "Epoch 7 Loss 2.9515 Accuracy 0.2059\n",
      "Epoch 8 Batch 0 Loss 2.7849 Accuracy 0.2156\n",
      "Epoch 8 Batch 50 Loss 2.7892 Accuracy 0.2118\n",
      "Epoch 8 Batch 100 Loss 2.7336 Accuracy 0.2139\n",
      "Epoch 8 Loss 2.7234 Accuracy 0.2142\n",
      "Epoch 9 Batch 0 Loss 2.5923 Accuracy 0.2198\n",
      "Epoch 9 Batch 50 Loss 2.5825 Accuracy 0.2186\n",
      "Epoch 9 Batch 100 Loss 2.5333 Accuracy 0.2215\n",
      "Epoch 9 Loss 2.5286 Accuracy 0.2216\n",
      "Epoch 10 Batch 0 Loss 2.4519 Accuracy 0.2250\n",
      "Epoch 10 Batch 50 Loss 2.4091 Accuracy 0.2258\n",
      "Epoch 10 Batch 100 Loss 2.3559 Accuracy 0.2289\n",
      "Epoch 10 Loss 2.3513 Accuracy 0.2290\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "for epoch in range(100):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "  \n",
    "    for (batch, (inp, tar)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        train_step(inp, tar)\n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: good morning.\n",
      "Predicted translation: ['<start>', 'quel', 'quel', 'porte', '!', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '!', '<end>', '<end>', '<end>', '<end>']\n"
     ]
    }
   ],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [1]\n",
    "    end_token = [2]\n",
    "  \n",
    "    sentence = preprocess_sentence(inp_sentence)\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    \n",
    "    encoder_input = tf.expand_dims(inputs, 0)\n",
    "  \n",
    "  # as the target is english, the first word to the transformer should be the\n",
    "  # english start token.\n",
    "    decoder_input = [1]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(max_length_targ):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "    # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == targ_lang_tokenizer.word_index[\"<end>\"]:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    # concatentate the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "\n",
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "  \n",
    "    sentence = inp_lang_tokenizer.encode(sentence)\n",
    "  \n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "  \n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "\n",
    "        ax.set_ylim(len(result)-1.5, -0.5)\n",
    "\n",
    "        ax.set_xticklabels(\n",
    "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "            fontdict=fontdict, rotation=90)\n",
    "\n",
    "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                            if i < tokenizer_en.vocab_size], \n",
    "                           fontdict=fontdict)\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "  \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def translate(sentence, plot=''):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "    predicted_sentence = ([targ_lang_tokenizer.index_word[i] for i in result.numpy()])  \n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence, result, plot)\n",
    "        \n",
    "translate(\"good morning.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
